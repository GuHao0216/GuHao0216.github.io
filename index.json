[{"categories":[],"content":"$\\|\\boldsymbol{x}\\|_{0}=\\sqrt[0]{\\sum_{i} x_{i}^{0}}$","date":"2022-04-23","objectID":"/first_post/:0:0","series":[],"tags":[],"title":"First_post","uri":"/first_post/#"},{"categories":null,"content":"一种基于Transformer的代码摘要方法","date":"2022-04-22","objectID":"/%E4%B8%80%E7%A7%8D%E5%9F%BA%E4%BA%8Etransformer%E7%9A%84%E4%BB%A3%E7%A0%81%E6%91%98%E8%A6%81%E6%96%B9%E6%B3%95/:0:0","series":null,"tags":null,"title":"一种基于Transformer的代码摘要方法","uri":"/%E4%B8%80%E7%A7%8D%E5%9F%BA%E4%BA%8Etransformer%E7%9A%84%E4%BB%A3%E7%A0%81%E6%91%98%E8%A6%81%E6%96%B9%E6%B3%95/#一种基于transformer的代码摘要方法"},{"categories":null,"content":"摘要生成描述程序功能的可读摘要称为源代码摘要。在这个任务中，通过对代码token之间的成对关系建模来学习代码表示，以捕获它们的长期依赖关系是至关重要的。为了学习用于摘要的代码表示，我们探索了使用自注意机制的Transformer模型，该模型在捕获长依赖关系方面已经证明是有效的。在这项工作中，我们表明，尽管方法很简单，但它比最先进的技术有显著的优势。我们进行了广泛的分析和控制变量研究，揭示了几个重要的发现，例如，源码token的位置障碍的绝对编码，而相对编码显著提高了摘要性能。我们已经公开了我们的代码，以方便未来的研究。 ","date":"2022-04-22","objectID":"/%E4%B8%80%E7%A7%8D%E5%9F%BA%E4%BA%8Etransformer%E7%9A%84%E4%BB%A3%E7%A0%81%E6%91%98%E8%A6%81%E6%96%B9%E6%B3%95/:1:0","series":null,"tags":null,"title":"一种基于Transformer的代码摘要方法","uri":"/%E4%B8%80%E7%A7%8D%E5%9F%BA%E4%BA%8Etransformer%E7%9A%84%E4%BB%A3%E7%A0%81%E6%91%98%E8%A6%81%E6%96%B9%E6%B3%95/#摘要"},{"categories":null,"content":"1 介绍程序理解是软件开发和维护不可缺少的组成部分(Xia et al.，2018)。源代码的自然语言摘要可以大大减少开发人员的工作，从而促进程序的理解(Sridhara等人，2010)。源代码摘要指的是创建可读的摘要来描述程序的功能。 随着深度学习技术和大量开源仓库对大规模数据的可用性的发展，自动源代码摘要已经引起了研究者们的关注。大多数基于神经网络方法都是按序列到序列的方式生成源代码摘要的。最初的工作之一Iyer等人(2016)训练了一个嵌入矩阵来表示单个的代码token，并通过注意机制将它们与循环神经网络(RNN)结合起来，生成自然语言摘要。后续研究(Liang and Zhu,2018;Hu et al.，2018a,b)在不同的代码摘要上采用了传统的基于RNN的序列到序列网络(Sutskever et al.，2014)和注意力机制(Luong et al.，2015)。 基于RNN的序列模型在学习源代码表示方面有两个局限性。首先，当它们按顺序处理代码token时，它们不会对源代码的非顺序结构进行建模。第二，源代码可能非常长，因此基于RNN的模型可能无法捕获代码之间的长依赖关系。与基于RNN的模型相比，利用自我注意机制的Transformer (Vaswani et al.，2017)可以捕获长依赖关系。Transformer在许多自然语言生成任务中表现良好，如机器翻译(Wang et al.，2019)、文本摘要(You et al.，2019)、故事生成(Fan et al.，2018)等。 为了学习序列中token的顺序或为token之间的关系建模，Trans-former需要注入位置编码(Vaswani等人，2017;Shaw等人，2018;Shiv和Quirk,2019)。在这项工作中，我们表明，通过使用相对位置表示对源代码token之间的两两关系建模(Shaw等人，2018)，相比于使用绝对位置表示学习代码token的序列信息有显著提升(Vaswani等人，2017)。 我们想要强调的是，我们提出的方法简单但有效，因为它的性能大大超过了花哨和复杂的最先进的源代码摘要技术。我们在两个从GitHub收集的较好的数据集上进行实验，结果证实了我们的方法相比最先进的方法更为有效。此外，我们还提供了详细的控制变量研究，以量化Transformer模型中几种设计选择的影响，为未来的研究提供坚实的基础。 源代码的自然语言摘要。代码和摘要都是由向量序列表示的token序列，$x=(x_1,\\ldots,x_n)$，其中$x_i\\in R^{d_{model}}$。在本节中，我们将简要描述Transformer的架构，以及如何在Transformer中对源代码标记的顺序或它们的成对关系进行建模。 ## 2 方法 我们提出使用Transformer(Vaswani等人,2017)生成给定一段源代码的自然语言摘要。代码和摘要都是由向量序列表示的token序列，$x=(x_1,\\ldots,x_n)$，其中$x_i\\in R^{d_{model}}$。在本节中，我们将简要描述Transformer的架构，以及如何在Transformer中对源代码标记的顺序或它们的成对关系进行建模。 ### 2.1 架构 Transformer由用于编码器和解码器的多层注意事项和参数化线性转换层组成。在每一层中，多头注意利用h个注意头，执行自注意力机制。 **自注意力机制：** 我们在Shaw等人(2018)的基础上描述了自注意力机制。在每个注意头中，输入向量的序列，$x=(x_1,\\ldots,x_n)$，其中$x_i\\in R^{d_{model}}$，被转换为输出向量序列，$o=(o_1,\\ldots,o_n)$，其中，$o_i\\in R^{d_k}$,为: $$o_i=\\sum_{j=1}^{n}{a_{ij}(x_jW^V)}，$$ $$e_{ij}=\\frac{x_iW^Q\\left(x_jW^K\\right)^T}{\\sqrt{d_k}},$$ 其中，$a_{ij}=\\frac{\\exp{e_{ij}}}{\\sum_{k=1}^{n}{exp{\\ e}_{ik}}}$，且$W^Q,W^K\\in R^{d_{model}\\times d_k}，W^V\\in R^{d_{model}\\times d_v}$，为每层和注意头唯一的参数。 **复制注意力：** 我们在Transformer中加入了复制机制(See等人,2017)，允许从词汇表生成单词，也可以从输入源代码中复制单词。我们使用一个额外的注意层来学习解码器堆栈上的副本分布(Nishida等人,2019)。复制注意力使Transformer能够从源代码中复制罕见的token(例如，函数名，变量名)，从而显著提高摘要性能。 ### 2.2 位置表示 现在，我们将讨论如何了解源代码标记的顺序或对它们的关系进行建模。 **编码绝对位置：** 为了使Transformer能够利用源代码令牌的顺序信息，我们训练一个嵌入矩阵$W^{P_e}$，该矩阵学习将令牌的绝对位置编码为维数$d_{model}$的向量。然而，我们发现捕获代码标记的顺序对学习源代码表示并没有帮助，并且会导致较差的摘要性能。 值得注意的是，我们训练了另一个学习对摘要标记的绝对位置进行编码的嵌入矩阵$W^{P_e}$。 **编码之间的关系：** 代码的语义表示并不依赖于符号的绝对位置。相反，它们之间的相互作用会影响源代码的含义。例如，表达式a+b和b+a的语义是相同的。 为了编码输入元素之间的成对关系，Shaw等人(2018)将自我注意机制扩展如下。 $$o_i=\\sum_{j=1}^{n}{a_{ij}\\left(x_jW^V+a_{ij}^V\\right)},$$ $$e_{ij}=\\frac{x_iW^Q\\left(x_iW^k+a_{ij}^k\\right)^T}{\\sqrt{d_k}},$$ 其中，$a_{ij}^V$和$a_{ij}^k$是两个位置$i$和$j$的相对位置表示。Shaw等人(2018)建议将最大相对位置剪切为$k$的最大绝对值，因为他们假设，在一定距离之外，精确的相对位置信息是没有用的。 $$a_{ij}^K=w_{clip\\left(j-i,k\\right)}^K,a_{ij}^V=w_{clip\\left(j-i,k\\right)}^V,$$ $$clip\\left(x,k\\right)=\\max(-k,\\min{\\left(k,x\\right)}).$$ 因此，我们学习了2k + 1个相对位置表示：$(w_{-k}^K,\\ldots,w_k^K)$和$(w_{-k}^V,\\ldots,w_k^V)$。 在这项工作中，我们研究了一种忽略方向信息的相对位置表示的替代方法(Ahmad等人，2019)。换句话说，忽略第$j$个token是在第$i$个token的左边还是右边的信息。 $$a_{ij}^K=w_{clip\\left(\\left|j-i\\right|,k\\right)}^K\\ ,\\ a_{ij}^V=w_{clip\\left(\\left|j-i\\right|,k\\right)}^V,$$ $$clip\\left(x,k\\right)=\\min(\\left|x\\right|,k).$$ ## 3 实验 ### 3.1 设置 **数据集和预处理。** 我们在Java数据集(Hu et al.，2018b)和Python数据集(Wan et al.，2018)上进行了实验。两个数据集的统计数据如表1所示。 ![](https://files.mdnice.com/user/25136/7031ab0a-bc49-497a-bb3d-2ba2fae9e436.png) 除了使用Wei等人(2019)的预处理步骤之外，我们还将驼峰命名法和蛇形命名法的源代码token拆分为各自的子token。我们证明这样的代码token分割可以提高摘要性能。 **评估指标。** 我们使用三个指标来评估代码摘要性能，BLEU (Papineni等人，2002年)、METEOR(Banerjee和Lavie,2005年)和ROUGE-L(Lin,2004年)。 基线方法。我们将基于Transformer的源代码摘要方法与Wei等人(2019)报告的五种基线方法及他们提出的对偶模型进行了比较。 **超参数。** 我们遵循Wei等人(2019)在这两个数据集中为代码和摘要设置了最大长度和词汇量。我们使用Adam优化器(Kingma和Ba,2015)对Transformer模型进行训练，初始学习率为 。我们将小批量尺寸和丢弃率分别设置为32和0.2。我们将Transformer模型训练为最多200个epoch，如果验证性能在连续20次迭代中没有改善，我们将提前停止。我们在推理过程中使用波束搜索，并将波束大小设置为4。详细的超参数设置可以在附录A中找到。 我们使用三个指标来评估代码摘要性能，BLEU (Papineni等人，2002年)、METEOR(Banerjee和Lavie,2005年)和ROUGE-L(Lin,2004年)。 **基线方法。** 我们将基于Transformer的源代码摘要方法与Wei等人(2019)报告的五种基线方法及他们提出的对偶模型进行了比较。 ### 3.2 结果分析 **整体结果。** 我们提出的模型和基线的总体结果在表2中给出。 ![](https://files.mdnice.com/user/25136/4348","date":"2022-04-22","objectID":"/%E4%B8%80%E7%A7%8D%E5%9F%BA%E4%BA%8Etransformer%E7%9A%84%E4%BB%A3%E7%A0%81%E6%91%98%E8%A6%81%E6%96%B9%E6%B3%95/:2:0","series":null,"tags":null,"title":"一种基于Transformer的代码摘要方法","uri":"/%E4%B8%80%E7%A7%8D%E5%9F%BA%E4%BA%8Etransformer%E7%9A%84%E4%BB%A3%E7%A0%81%E6%91%98%E8%A6%81%E6%96%B9%E6%B3%95/#1-介绍"}]